In our setup, we follow a branching model aligned with our CI/CD environments.
Developers work on feature/* branches â€” Jenkins only builds, tests, and runs SonarQube and Trivy scans.
Once merged to develop, Jenkins builds the Docker image, pushes it to ECR, updates the Helm chart, and ArgoCD automatically syncs it to our staging EKS environment.
After testing, code is merged to main, which triggers the same pipeline but deploys to production.

This way, our deployment is completely automated, environment-specific, and follows GitOps principles through ArgoCD



order
cart
checkout
payment

each microservice is having different codebase
for each and evry microservice there is seperate github repository.
each microservice developed by different team
or different set of developers.
each microservice is hvaing there own repo ,own build process ,own deployment process its going all individually.
for each and every repository we have one dockerfile and jenkinsfile 



How you will HA architecture on AWS.

--> Purpose- To design HA architecture in such way that our application will continue available if any server ,
     AZ , aws service fail.

compute layer --> To achieve high availability we placed our application servers into multiple AZ with auto-scaling                   enabled.
                  if anyone zone goes down then traffic will get switch to healthy instance in the other AZ.
                  I will place load balancer in front of application servers for traffic distributions and failover.
                  load balancer continuously monitor health check of registerted target and send traffic to healthy                   instance. 
                  
database layer   To achieve high availability at database layer we use amazon RDS multi AZ deployment or aurora with
                 cross-AZ replicas. in primary region database handle read and write operation. these database 
                  maintain synchronous replication across  multiple AZ inside the same region for durability. 
                 aurora reader continuously receives asynchronous replication from the primary aurora writer.
                   failover happens automatically within sec. if the primary database becomes unavailable.
  
networking layer - we will configure VPC with multiple public and private instance 
storage layer


dev   - 1 master   8 worker node

test  - 3 master   22 worker node

 prod  - 3 master  22 worker node

storage solution for EKS Cluster- 
EFS -NAS storage 
EBS -SAN Storage 
Network - VPC CNI
Deployment - we used helm as deployment method.


which algorithm working in bakcned when ectd sycn in beetwen.
i have 3 master node what is the command to know which node is leader and member.
why always odd numbering follwoed in cluter 


api service , etcd, controller , scheduler they are running on staic pod concept

grep -i manifests  /var/lib/kubelet/config.yaml





                                vpc (10.0.0.0/16)
                                       |
              ++++++++++++|+++++++++++++++++++++++++|+++++++
              |                        |                   |
         public subnet                            private subnet
     vpc_id=aws_vpc.main.id                     vpc_id=aws_vpc.main.id
              | 10.0.1.0/24                          10.0.2.0/24
              |                                            |
              +                                            +                       
              |                                            | 
              |                                            |                    
              |                                            |
           internet gateway                               NAT Gateway
           vpc_id=aws_vpc.main.id                         subnet_id = aws_subnet.public_subnet.id
              |                                            |
          public rt                                    private rt
    vpc_id=aws_vpc.main.id                               vpc_id=aws_vpc.main.id
 gateway_id = aws_internet_gateway.production-igw.id     nat_gateway_id = aws_nat_gateway.production-nat.id
         routes:- 10.0.0.0/16-> local                    routes:- 10.0.0.0/16->local
                  0.0.0.0 -> IGW                                   0.0.0.0 -> NAT GW
              |                                            |
      public route table association                 private route table association
             ||                                            ||
  subnet_id=aws_subnet.public_subnet.id                 subnet_id=aws_subnet.private_subnet.id
  route_table_id = aws_route_table.public_rt.id         route_table_id = aws_route_table.private_rt.id -->

