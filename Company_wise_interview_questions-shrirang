
===========
rapidsoft
===========

How much you are comfortable you are with the CICD?
Which cicd platform you used?
Securing credintials inside the cicd how will you do?
copy file from local to remote and remote to local how will you do?
how you manage the state file 
how do you manage branching system?
give me the brief introduction on project you have done?
how nodes communicated with each other?
how will you define ingress/egress inside the deployment file? 
what is the difference in bewteen env run and CMD?
Have you used helm chart what do you do on it?
what and when will you use terraform workspace?
job role you daily perform on daily basis?


===========
Altimetrik
===========

1) s3 bucket having 100GB data i  have requirement to move this data different s3 into different region so how do you do that?
--> I have to move data to destination bucket so i will use cross region replication. CRR will automatically copy all existing and future data from 
source bucket to destination in different 
AWS refion.
steps:--I will enable bucket versioning on both bucket because replication requires versioning. 
        1️⃣ Create destination bucket in target region and enable versioning on both buckets.
2️⃣ Create an IAM role on the source side with a trust policy for S3 and permission policy that allows:
GetObjectVersion* on source bucket
Replicate*, PutObject on destination bucket
3️⃣ Then I’ll configure replication on the source bucket using that IAM role ARN.
4️⃣ Once done, S3 automatically assumes that role and replicates new objects across regions
        Then i will create IAM role into source bucket account so s3 will assume this role to perform replication.

2) i have specific file in s3 and someone has deleted out of s3 how do you identify who deleted this file? /any way to recover
-->  whenever any deletion happens in aws service resources then it will get reflected to cloudtrail logs. cloudtrail is having records of 
     DeleteObject API event along with username, IAM role, source IP, and timestamp.
     If versioning is enabled in s3 on bucket then i can simply restore deleted file/object by removing delete marker or retrieving the older 
     version.
    if versioning is not enabled then recovery is not possible, but i will recommend enabling s3 versiioning and MFA delete going forward to
   prevents such events.
       i also setup cloudwatch alarms on DeleteObjects events to get instant notifications if any deletion happens.


3) i hav ec2 instance i accidently loss my pem file is any way to get it into ec2 instance?


application in public subnet needs to access database which is in private subnet so how do you establish that connection?
do you know crosszone load balancing?
what is the sticky session in load balancing concepts?
how frequently you are doing your deployments?
   We follow a continuous deployment approach for our non-prod environments and a controlled release cycle for production.
Code changes from developers are deployed to the dev environment daily through automated Jenkins + ArgoCD pipelines.
Once validated, the same build is promoted to staging for QA and UAT, usually 2–3 times per week.
Production releases happen once every two weeks, depending on business approval.
Since our entire pipeline is automated — build, scan, test, and deploy — we can release on-demand anytime with minimal manual intervention.”

which deployment strategy provides kubernetes?
Q) After deployment some of the pods  stuck in pending state so what could be the reason?
--> pod sheduling failed on nodes because scheduler  unable to place that pods onto nodes due to some reasons.
  Common root causes- 1) I will check events sections of pods using kubectl describe pod pod-name -n <namespace>
a) Insufficient CPU-  solution--> we need to scale up nodes.//reduce resource.requests in the deployment yaml 
b) node didnt match node selector -> sometimes pod is configured to run on specific node if this node unavailable then pod stays pending. 
c) persistent volume issue- if the requested pvc not present for pod then pod stays pending.-Need to check storage class. verify EBS is in same AZ
d) Namespace doent have enough quota- kubectl get quota -n <namespace> --> need to increase quota
c) Nodes didnt have the free ports for the requested pod.















do you know whats are probes available and how do we use them?
  --Yes, Kubernetes provides three types of probes — liveness, readiness, and startup probes.
Liveness probe checks if the container is running properly. If it fails, Kubernetes automatically restarts the container.
Readiness probe checks if the container is ready to accept traffic. Until it passes, the pod won’t receive requests from the Service.
Startup probe is used for slow-starting applications — it ensures the container finishes initialization before liveness or readiness checks 
start.
We can define these probes using httpGet, tcpSocket, or exec commands inside the pod spec.
By configuring these probes properly, we ensure self-healing behavior in Kubernetes and zero downtime deployments — since traffic always goes
only to ready and healthy pods


i wan to establish communication between pods inside node and inside the cluster not should be able to communicate trhough other node how do you do 
that
i want to create 2 ec2 instance using terraform in 3 env.( dev, testing, prod ) and i dont want to create 3 different files (dev.tf, testing.tf..)  
i just want one main.tf and run it .when i run terraform apply 
terraform should create resources in all 3 of my environemnt how to do that?
how do you mention environments in terraform?
what are the provisioners in terraform?
we  both working on same project and we both apply terraform apply on same resource and terraform state corrupted how do we fix this?
please explain what you have done using helm?
