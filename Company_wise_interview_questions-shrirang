
===========
rapidsoft
===========

How much you are comfortable you are with the CICD?
Which cicd platform you used?
Securing credintials inside the cicd how will you do?
copy file from local to remote and remote to local how will you do?
how you manage the state file 
how do you manage branching system?
give me the brief introduction on project you have done?
how nodes communicated with each other?
how will you define ingress/egress inside the deployment file? 
what is the difference in bewteen env run and CMD?
Have you used helm chart what do you do on it?
what and when will you use terraform workspace?
job role you daily perform on daily basis?


===========
Altimetrik
===========

1) s3 bucket having 100GB data i  have requirement to move this data different s3 into different region so how do you do that?
--> I have to move data to destination bucket so i will use cross region replication. CRR will automatically copy all existing and future data from 
source bucket to destination in different 
AWS refion.
steps:--I will enable bucket versioning on both bucket because replication requires versioning. 
        1️⃣ Create destination bucket in target region and enable versioning on both buckets.
2️⃣ Create an IAM role on the source side with a trust policy for S3 and permission policy that allows:
GetObjectVersion* on source bucket
Replicate*, PutObject on destination bucket
3️⃣ Then I’ll configure replication on the source bucket using that IAM role ARN.
4️⃣ Once done, S3 automatically assumes that role and replicates new objects across regions
        Then i will create IAM role into source bucket account so s3 will assume this role to perform replication.

2) i have specific file in s3 and someone has deleted out of s3 how do you identify who deleted this file? /any way to recover
-->  whenever any deletion happens in aws service resources then it will get reflected to cloudtrail logs. cloudtrail is having records of 
     DeleteObject API event along with username, IAM role, source IP, and timestamp.
     If versioning is enabled in s3 on bucket then i can simply restore deleted file/object by removing delete marker or retrieving the older 
     version.
    if versioning is not enabled then recovery is not possible, but i will recommend enabling s3 versiioning and MFA delete going forward to
   prevents such events.
       i also setup cloudwatch alarms on DeleteObjects events to get instant notifications if any deletion happens.


3) i hav ec2 instance i accidently loss my pem file is any way to get it into ec2 instance?


Q) application in public subnet needs to access database which is in private subnet so how do you establish that connection?
-->  Both instances are in same VPC, so they can communicate via their private ips.
     I ensure route tables allow internal communication , and the DB security group allow inbound connections from the application security
     group on the database port. The connection performs within VPC, - No traffic goes to public internet.

do you know crosszone load balancing?
    Cross-Zone Load Balancing allows an AWS Load Balancer to distribute incoming traffic evenly across all targets in all Availability Zones.
Without it, each load balancer node only sends traffic to targets in its own AZ, which can cause uneven load if target counts differ between AZs.
Enabling it improves availability and performance — ALB has it enabled by default, while NLB and CLB can be configured to use it.
For NLBs, there’s a small inter-AZ data transfer cost to consider

what is the sticky session in load balancing concepts?
Q)  how frequently you are doing your deployments?
     we followed continouous deployment approach for our non-production environments and controlled release cycle for production.
   code changes form developers are developed to dev environment daily through automated jenkins+Argocd pipelines.
    once validated , the same build is promoted to staging for QA and UAT , usually 2-3 times per week.
    production release happens once every two weeks , depending on business approval.
    since our entire pipeline is automated - build, scan test, deploy - we can release on-demand anytime with minimal manual intervention.

which deployment strategy provides kubernetes?
Q) After deployment some of the pods  stuck in pending state so what could be the reason?
--> pod sheduling failed on nodes because scheduler  unable to place that pods onto nodes due to some reasons.
  Common root causes- 1) I will check events sections of pods using kubectl describe pod pod-name -n <namespace>
a) Insufficient CPU-  solution--> we need to scale up nodes.//reduce resource.requests in the deployment yaml 
b) node didnt match node selector -> sometimes pod is configured to run on specific node if this node unavailable then pod stays pending. 
c) persistent volume issue- if the requested pvc not present for pod then pod stays pending.-Need to check storage class. verify EBS is in same AZ
d) Namespace doent have enough quota- kubectl get quota -n <namespace> --> need to increase quota
c) Nodes didnt have the free ports for the requested pod.

Q) Do you know whats are probes available and how do we use them?
  --Yes, Kubernetes provides three types of probes — liveness, readiness, and startup probes.
Liveness probe checks if the container is running properly. If it fails, Kubernetes automatically restarts the container.
Readiness probe checks if the container is ready to accept traffic. Until it passes, the pod won’t receive requests from the Service.
Startup probe is used for slow-starting applications — it ensures the container finishes initialization before liveness or readiness checks 
start.
We can define these probes using httpGet, tcpSocket, or exec commands inside the pod spec.
By configuring these probes properly, we ensure self-healing behavior in Kubernetes and zero downtime deployments — since traffic always goes
only to ready and healthy pods


i wan to establish communication between pods inside node and inside the cluster not should be able to communicate trhough other node how do you do 
that
i want to create 2 ec2 instance using terraform in 3 env.( dev, testing, prod ) and i dont want to create 3 different files (dev.tf, testing.tf..)  
i just want one main.tf and run it .when i run terraform apply terraform should create resources in all 3 of my environemnt how to do that?
how do you mention environments in terraform?

Q) what are the provisioners in terraform?
Provisioners in Terraform are used to perform actions on a resource after it’s created — like installing software, running configuration scripts, 
or executing commands inside the instance.
Common provisioners include remote-exec, local-exec, and file.
remote-exec runs commands on the remote resource — for example, installing Jenkins or Docker on an EC2 instance after creation.
local-exec runs commands on the local machine where Terraform is executed — for example, triggering an Ansible playbook or Jenkins job.
file copies files from the local system to the remote instance.
For example, in my projects, I used a remote-exec provisioner to install DevOps tools automatically on EC2 after provisioning.
But in general, we try to avoid overusing provisioners — instead, we use configuration management tools like Ansible, or custom AMIs, because Terraform’s primary focus is on infrastructure provisioning, not software setup

we  both working on same project and we both apply terraform apply on same resource and terraform state corrupted how do we fix this?
-->> When both persons working on same projects and they applied terraform apply at same time then state file at local/remote will get corrupt.
     so first step need to take is to stop both task execution. and we need to inspect manually and restore the valid state from backup.-either from
the .terraform local backup or s3 remote backend version.
 Then i'd initialize working directory with terraform init , run terraform refresh to sync the state with real infrastructure, and validate using
 terraform plan. finally to avoid this issue permentaly i will configure remota backend with state locking.
mv terraform.tfstate.backup terraform.tfstate   --> terraform init -->terraform refresh --> terraform plan




please explain what you have done using helm?
