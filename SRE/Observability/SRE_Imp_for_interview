
SLI (Service Level Indicator) is basically a metric â€” it tells us how well a service is performing. For example, metrics like 
availability, latency, error rate, or throughput are SLIs. Itâ€™s the actual measured value. For instance, if 99.5% of API requests 
are successful in a month, that 99.5% is the SLI.

SLO (Service Level Objective) is the target or goal for that metric. It defines what level of reliability we want to achieve. 
For example, our SLO might state that 99.9% of API requests should succeed per month.

SLA (Service Level Agreement) is a formal contract or agreement between the service provider and the customer. It includes the 
SLO, but also defines penalties or compensations if the target isnâ€™t met. For instance, if uptime goes below 99.9%, the customer 
might get a service credit or refund.

So in short:
ğŸ‘‰ SLI â€“ What we measure (e.g., availability = 99.7%)
ğŸ‘‰ SLO â€“ What we aim for (target = 99.9%)
ğŸ‘‰ SLA â€“ What we promise in writing (contract = 99.9% uptime or we pay penalties)

Example scenario:
If we have an e-commerce API, we might define:

SLI = percentage of successful API calls

SLO = 99.9% success over 30 days

SLA = if uptime drops below 99.9%, we credit the customer 5% of monthly billing

Thatâ€™s how all three connect â€” SLI feeds into SLO, and SLO becomes part of the SLA.â€

--------------------------------------------------------------------------------------------------------------------------------

ğŸ¯ Main Reliability & Incident Metrics
1ï¸âƒ£ MTTA â€” Mean Time To Acknowledge

â€œMTTA means Mean Time To Acknowledge.
It measures how long it takes for the team to acknowledge an alert after itâ€™s triggered.

Basically â€” when an incident occurs, how quickly did we respond?

ğŸ‘‰ Formula:

MTTA
=
                    Total time from alert to acknowledgment
              MTTA= -------------------------------------------
                       Number of incidents


Total time from alert to acknowledgment
	â€‹


Example:
If an alert is triggered at 10:00 and the engineer acknowledges it at 10:05, MTTA = 5 minutes.

So, lower MTTA = faster human response, which means your monitoring and on-call alerting process is effective.â€

2ï¸âƒ£ MTTR â€” Mean Time To Resolve (or Repair or Recover)

â€œMTTR stands for Mean Time To Resolve.
It measures the average time it takes to restore normal service after an incident occurs.

ğŸ‘‰ Formula:

                      Total downtime durationâ€‹
              MTTR= ------------------------
                       Number of incidents

	â€‹


Example:
If a service went down at 10:00, acknowledged at 10:05, and was fixed at 10:25 â€” the MTTR is 25 minutes.

This metric shows how quickly your team can identify, fix, and restore the system.
Reducing MTTR means your incident management and automation are improving.â€

3ï¸âƒ£ MTTD â€” Mean Time To Detect

â€œMTTD is Mean Time To Detect â€” how long it takes to notice an issue after it actually occurs.

ğŸ‘‰ Formula:

                       Total detection time
              MTTD= ------------------------
                       Number of incidents
	â€‹


Example:
If a database outage starts at 10:00 but your monitoring detects it at 10:03, MTTD = 3 minutes.

The goal is to minimize MTTD using strong observability â€” like Prometheus, Dynatrace, or CloudWatch alerts.â€

4ï¸âƒ£ MTBF â€” Mean Time Between Failures

â€œMTBF means Mean Time Between Failures.
It tells us how reliable a system is â€” basically, the average time the service runs smoothly before it fails again.

ğŸ‘‰ Formula:

                          Total uptime
              MTBF= ------------------------
                       Number of failures
	â€‹

Example:
If your service ran for 200 hours before a failure, and this pattern repeats, MTBF = 200 hours.
A higher MTBF means your system is more stable and reliable.â€

5ï¸âƒ£ MTTF â€” Mean Time To Failure

â€œMTTF stands for Mean Time To Failure.
This is used for non-repairable systems â€” basically how long a system or component works before it permanently fails.

Example: hardware devices like a hard disk â€” if on average a disk runs for 4 years before failing, MTTF = 4 years.â€

âš¡ Bonus Reliability Term
6ï¸âƒ£ Error Budget

â€œThis is a very common follow-up when you talk about SLOs.
Once you define an SLO â€” say 99.9% availability â€” it means you have 0.1% of downtime allowed per month.
That allowed downtime is your error budget.

ğŸ‘‰ Example:
If there are 43,200 minutes in a month,
0.1% = 43 minutes of downtime allowed.

If we exceed that, it means weâ€™ve consumed the full error budget â€” and that often triggers actions like halting new releases or 
improving reliability before proceeding further.â€

----------------------------------------------------------------------------------------------------------------------------------


Toil -
â€œToil is manual, repetitive, and automatable work that doesnâ€™t add long-term value.
For example, manually restarting pods, creating users, or running deployment scripts by hand â€” all are toil.
SREs aim to reduce toil through automation â€” by using Jenkins, Ansible, Terraform, or Kubernetes operators.
Google suggests keeping toil under 50% of an SREâ€™s time. That way, the rest can go to engineering and improving reliability.â€

----------------------------------------------------------------------------------------------------------------------------------
Blameless Postmortems - 
â€œA blameless postmortem is a process to analyze incidents without blaming individuals.
The focus is on understanding what happened, why it happened, and how to prevent it in the future.
Key principles:
1. No blame: Focus on system failures, not people.
2. Psychological safety: Encourage open discussion without fear of punishment.
3. Actionable outcomes: Identify specific steps to improve systems and processes.
4. Transparency: Share findings with the team to foster learning.
5. Continuous improvement: Use insights to enhance reliability and prevent future incidents.
Example structure of a blameless postmortem:
- Incident summary
- Timeline of events
- Root cause analysis
- Contributing factors
- Lessons learned
- Action items for improvement
By following these principles, teams can learn from failures and improve system reliability without creating a culture of fear or blame.â€

Blameless postmortems promotes a culture of trust and Continuous improvement, which is a core SRE principle.

