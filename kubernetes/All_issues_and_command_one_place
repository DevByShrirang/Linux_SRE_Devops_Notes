controlplane:~$ kubectl get componentstatus
Warning: v1 ComponentStatus is deprecated in v1.19+
NAME                 STATUS    MESSAGE   ERROR
etcd-0               Healthy   ok        
scheduler            Healthy   ok        
controller-manager   Healthy   ok        

controlplane:/etc/kubernetes/manifests$ ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml
controlplane:/etc/kubernetes/manifests$ sudo mv /etc/kubernetes/manifests/kube-scheduler.yaml /root/
controlplane:/etc/kubernetes/manifests$ ls
etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml
controlplane:/etc/kubernetes/manifests$ cd          
controlplane:~$ kubectl run testpod --image=nginx
pod/testpod created
controlplane:~$ kubectl get pods                 
NAME      READY   STATUS    RESTARTS   AGE
testpod   0/1     Pending   0          11s

controlplane:~$ sudo mv /root/kube-scheduler.yaml /etc/kubernetes/manifests/
controlplane:~$ kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
testpod   0/1     ContainerCreating   0          6m47s
controlplane:~$ kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
testpod   1/1     Running   0          6m58s


If the scheduler goes down or becomes unhealthy, the existing running workloads are not affected — because those pods are already managed by the kubelet on 
each node.
However, any new pods that are created or need rescheduling (like from a deployment, job, or replica set) will remain in the Pending state, because there’s 
no active scheduler to assign them to a node.”

✅ Key point: running pods continue, new pods stuck as Pending.



--------------------------------------------------------------------------------------------------------------------------------------------------------------
Pod status is in pending state. (Node lebel not matching with selector)
kubectl describe pod  --> warning - failed scheduling  default scheduler  0/4 nodes are available  1 nodes had untolerated taint 
(node-role.kubernetes.io/control plane). 3 nodes didnt match pods node affinity/selector

Above error shows that there are 4 nodes available. pod is having label something that was not present on node specification. so pod is not able to place on any node.

as we viewved in manifiest yaml there is nodeSelector defined for these pods. 
 nodeSelector:
    disk: hdd

Now to check whether we have assigned label or not to any node.
kubectl get nodes -l disk=hdd
no resource found --> Means node is not having label hdd. so no any pod scheduling on that node.

We will assign label to node.
kubectl label node worker-2 disk=hdd

---------------------------------------------------------------------------------------------------------------------------------------------------------------

Error :-
kubectl describe pod  --> warning - failed scheduling  default scheduler  0/4 nodes are available  1 nodes had untolerated taint 
(node-role.kubernetes.io/control plane). 3 nodes had untolerated taint {red:  }.  preemption: 0/4 node are available : 4 prremption is not helpful for scheduling.

Above error says that 3 nodes having same taint red. we need to check whether toleration present or not in manifiest.

kubectl describe node worker-1 | grep -i taint
Taints:  red:NoSchedule

Above command shows that above node is having taint so we need add toleration for pod if not present /mismatch.

spec:
  tolerations:
    - key: red
      effect: NoSchedule
      operator: Exists


if we have not mentioned any operator then it will take default operator.
Below are the commands to untaint the nodes.
kubectl taint node <node_name> red:NoSchedule-
kubectl edit node node_name

---------------------------------------------------------------------------------------------------------------------------------------------------------------------

i have 3 worker Nodes i want to delpoy container. the choice of deployment will be node-2. if node-2 is not available then he can schedule on node3.

preferred duringscheduling ignored during execution
required duringscheduling ignored during execution
























