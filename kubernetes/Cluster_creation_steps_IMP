We have automated EKS cluster creation using terraform IAC.
we configured both control plane and worker node component through terraform. we need to set up IAM role, networking, OIDC Provider.
we need to define provider block with specific region.

we have to create 2 IAM role for Cluster and worker node components

IAM Role for EKS Control plane:-  
eks.amazonaws.com
we have to attach policies to control plane 
AmazonEKSClusterPolicy - EKS will manage cluster components.
AmazonEKSServicePolicy -  Using this policy EKS will interact with other services.
AmazonEKSVPCResourceController - To manage VPC rrsources like ENI.'

VVIMP:- when we create EKS cluster we pass IAM role ARN to control plane via role_arn parameter. so trust policy will allow eks.amazonaws.com to assume this role to get permissions to
       manage cluster related resources.


IAM Role for worker node-
ec2.amazonaws.com
AmazonEKSWorkerNodePolicy
AmazonEKS_CNI_Policy (for VPC CNI networking)
AmazonEC2ContainerRegistryReadOnly (for pulling images from ECR)
AmazonSSMManagedInstanceCore (for SSM access)
AmazonS3ReadOnlyAccess (if pods need to read from S3)
Plus a custom AutoScaler policy that lets the cluster autoscaler scale EC2 nodes.”

Finally, I create an IAM instance profile to attach this role to the worker EC2 instances. instance profile allows us each worker node to assume the  role automatically at startup
    without needing to configure credenntials nually.


the resource aws_eks_cluster creates the EKS control plane. i also use depends_on to make sure all IAM attachements are created before the cluster.
The resource aws_eks_node_group creates managed node group.
 since its managed node group , AWS automatically picks the right EKS-optimized AMI so dont need to specify AMI manually.


OIDC provider

"**Cluster join process in EKS**":-
   worker nodes associated with control plane through cluster join process that uses aws-auth configmap. 
   this configmap maps the worker node IAM role to kubernetes RBAC permissions, allowing the control plane to trust and register those nodes.
    once this mapping exists , the kubelet on each node authenticates to the EKS API server using the instance IAM role credintials and successfully joins the cluster.

EKS uses IAM-based authentication for worker nodes. without IAM role EKS will not allow authentication of worker node.


4️⃣ The aws-auth ConfigMap (in kube-system namespace)

It looks like this (Terraform can create it or AWS does it automatically for managed node groups):

admin@DESKTOP-3I3NJDG MINGW64 ~/.kube (shri)
$ kubectl get configmap -n kube-system
NAME                                                   DATA   AGE
amazon-vpc-cni                                         7      43m
aws-auth                                               1      41m
coredns                                                1      43m
extension-apiserver-authentication                     6      44m
kube-apiserver-legacy-service-account-token-tracking   1      44m
kube-proxy                                             1      43m
kube-proxy-config                                      1      43m
kube-root-ca.crt                                       1      43m

admin@DESKTOP-3I3NJDG MINGW64 ~/.kube (shri)
$ kubectl get configmap aws-auth -n kube-system -o yaml
apiVersion: v1
data:
  mapRoles: |
    - rolearn: arn:aws:iam::442042505508:role/shrirang-eks-worker1
      groups:
      - system:bootstrappers
      - system:nodes
      username: system:node:{{EC2PrivateDNSName}}
kind: ConfigMap
metadata:
  creationTimestamp: "2025-10-27T07:22:34Z"
  name: aws-auth
  namespace: kube-system
  resourceVersion: "822"
  uid: 234ab42b-8744-40f5-bed4-278693b3e32f

above configmap tells EKS any EC2 Instance that assumes this IAM role should be trusted and allowed to register as kubernetes nodes.
when worker node kubelet connect to API server using its IAM credintials , the API sever check its IAM role listed in aws-auth or not.
  if yes --> Register the node and mark it as reddy.
      No --> Reject the request.

Kubelet registration process

The kubelet on the worker node contacts the EKS API server using the endpoint and cluster CA certificate.
These details are stored in /etc/eks/bootstrap.sh when the instance boots.

The kubelet authenticates using a signed request via AWS IAM Authenticator (aws-iam-authenticator).

The control plane verifies the IAM role against the aws-auth ConfigMap.

Once authorized, the node gets added to the cluster and appears in:




admin@DESKTOP-3I3NJDG MINGW64 /d/final_interview_call_cicd/Microservices-E-Commerce-eks-project01/eks-terraform (master)
$ aws eks list-clusters  --region us-east-2
{
    "clusters": [
        "project-eks"
    ]
}
admin@DESKTOP-3I3NJDG MINGW64 /d/final_interview_call_cicd/Microservices-E-Commerce-eks-project01/eks-terraform (master)
$ aws eks describe-cluster --name project-eks --region us-east-2 --query "cluster.status"
"ACTIVE"

admin@DESKTOP-3I3NJDG MINGW64 /d/final_interview_call_cicd/Microservices-E-Commerce-eks-project01/eks-terraform (master)
$ aws eks update-kubeconfig --name project-eks --region us-east-2
Updated context arn:aws:eks:us-east-2:442042505508:cluster/project-eks in C:\Users\admin\.kube\config



aws eks update-kubeconfig creates or updates kubeconfig file under ~/.kube/config, merge in cluster details,  configure aws iam based authentication, and allows kubectl to communicate securely with EKS API server.


------------------------------------------------------------------------

when worker node joined cluster--> kubelet used IAM role attached to node to request a token from AWS STS. that token was validated by EKS control plane against the aws-auth configmap. once validated node was successfully registered 
with the control plane. after registration kubelet maintains persistent , long lived connection to the API server using client certificates, and open gRPC Stream. so once node registration is done kubelet doesnt constantly  ccheck 
IAM role mappings. aws-auth is used only when authenticating initially not continuously.
when node joined control plane issued a client certificate to the kubelet. that certificate is valid for about one year in eks. so as long as certifications remains valid and connection is healthy the node keeps functioning - even if the
IAM role mapping in aws-auth changed or removed.

Each worker node in EKS runs a kubelet that connects securely to the API server.
During the bootstrap process, the kubelet authenticates using the node’s IAM role via aws-iam-authenticator, which checks the aws-auth ConfigMap.
Once verified, the control plane issues a client certificate stored locally under /var/lib/kubelet/pki/.
The kubelet then uses this certificate for all future communication, which is why removing the role from aws-auth doesn’t immediately disconnect existing nodes — the certificate is still valid until it expires or the node restarts.”


X.509 certificate present at below location on every node.
/var/lib/kubelet/pki/kubelet-client-current.pem OR
/var/lib/kubelet/pki/kubelet-client.crt
/var/lib/kubelet/pki/kubelet-client.key












































